<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://lingo-iitgn.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lingo-iitgn.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-25T03:36:06+00:00</updated><id>https://lingo-iitgn.github.io/feed.xml</id><title type="html">blank</title><subtitle>Computational Linguistics and Complex Social Networks Group. </subtitle><entry><title type="html">student travel grants</title><link href="https://lingo-iitgn.github.io/blog/2023/travel/" rel="alternate" type="text/html" title="student travel grants"/><published>2023-10-20T14:30:00+00:00</published><updated>2023-10-20T14:30:00+00:00</updated><id>https://lingo-iitgn.github.io/blog/2023/travel</id><content type="html" xml:base="https://lingo-iitgn.github.io/blog/2023/travel/"><![CDATA[<p>Here is the list of travel grants that you can apply for travelling to international conferences.</p> <ol> <li>Google India: <a href="https://buildyourfuture.withgoogle.com/scholarships/google-conference-scholarships">Google</a></li> <li>Microsoft India Research: <a href="https://www.microsoft.com/en-us/research/academic-program/academic-outreach/">MSR</a></li> <li>ACM India: <a href="https://www.iarcs.org.in/activities/grants.php">IARCS</a></li> <li>SERB India: <a href="https://www.serbonline.in/SERB/its">SERB</a></li> <li>IIT Gandhinagar: It is there always, it can work as a backup, reach out to Academic office for the details</li> </ol>]]></content><author><name>Pritam Kadasi</name></author><category term="academics"/><category term="travel"/><summary type="html"><![CDATA[no funds to travel?]]></summary></entry><entry><title type="html">how to run your first job on paramananta?</title><link href="https://lingo-iitgn.github.io/blog/2023/paramananta/" rel="alternate" type="text/html" title="how to run your first job on paramananta?"/><published>2023-10-20T13:30:00+00:00</published><updated>2023-10-20T13:30:00+00:00</updated><id>https://lingo-iitgn.github.io/blog/2023/paramananta</id><content type="html" xml:base="https://lingo-iitgn.github.io/blog/2023/paramananta/"><![CDATA[<h2 id="instructions">Instructions</h2> <p>You will have to run the below command:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sbatch fname.sh
</code></pre></div></div> <p>and the fname.sh looks like below</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

#SBATCH --job-name=mbert-gpu
#SBATCH --partition=gpu
#SBATCH --gres=gpu:2
#SBATCH --nodes=1                	# Run all processes on a s.node 	 
#SBATCH --ntasks=1               	# Run a single task           	 
#SBATCH --cpus-per-task=32        	# Number of CPU cores per tasks
#SBATCH --time=03:00:00          	# Time limit hrs:min:sec
#SBATCH --output=../logs/mbert_%j.log 	# Standard output and error log

pwd; hostname; date

echo "fine-tuning mbert on $SLURM_CPUS_ON_NODE CPU cores"

python ../py/mbert_param.py

Date
</code></pre></div></div> <p><strong>#SBATCH –time=03:00:00</strong> ⇒ if this time exceeds, your job will terminate immediately, so please give more time in order to avoid termination. <em>Max-time limit on GPU partition is 2 days</em>, You can check the time limit of all partitions using command “<strong>sinfo</strong>”</p> <p>There is a limited amount of storage available per user, it’s ~50GB,it may even decrease further as the number of users increases.</p> <p>In the above .sh file, you can play with the parameters, if any parameter is not correct, then it will throw an error. You can always check the log file.</p> <p>This is enough to get started.</p> <p>FOR GPU: there are only two options available here [<strong>#SBATCH –gres=gpu:2</strong>] (it will be only 1 or 2); #GPUs per node (Max. 2). For a GPU node, it has two GPU cards, each of 16GB.</p> <p>For submitting the job: <code class="language-plaintext highlighter-rouge">sbatch fname.sh</code></p> <p>For checking job queue: <code class="language-plaintext highlighter-rouge">squeue</code></p> <p>For terminating the job: <code class="language-plaintext highlighter-rouge">scancel job_id [you can get job id from squeue]</code></p> <p>To run a job on a specific node: <code class="language-plaintext highlighter-rouge">#SBATCH --nodelist=gpu0xx</code></p> <p>Show specific job details: <code class="language-plaintext highlighter-rouge">scontrol show job job_id</code></p> <p>Using the “<strong>sinfo</strong>” command, you can check the partition on which you want to run.</p> <p>As quoted by admin:</p> <blockquote> <p>As per policy at a given instance maximum 6 jobs can be in running state at a time under “GPU” partition within the same group. Rest will be in idle state.</p> </blockquote> <blockquote> <p>One group: IITGn, another group: non-IITGn</p> </blockquote> <p><img src="/assets/img/blogs/param.png" alt="squeue" title="squeue command display"/></p> <p>After submitting your job, it can be seen in the job queue, so for user “suraj.k” who is running his job on “gpu002”, he can check his gpu usage after logging into the gpu node; like below.</p> <p><code class="language-plaintext highlighter-rouge">ssh gpu002</code></p> <p>After logging into the node, he can run the “nvidia-smi” command. So you can also try it out on your submitted job. You can’t try it out on other users’ jobs, you will get permission denied error.</p> <p>you don’t have to make any changes in the python file. Only changes are to be made in the .sh file and we should mention the path of the python file in the .sh file.</p> <h2 id="multi-gpu">Multi-GPU</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

#SBATCH --job-name=muril-mu-gpu
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1               # GPUs per node (Max. 2)[2]
#SBATCH --nodes=2                  # Run all processes on a multi node       
#SBATCH --ntasks=2                 # Run multi task [4]
#SBATCH --tasks-per-node=1      	  # Tasks per node  [2]          
#SBATCH --cpus-per-task=32      	  # Number of CPU cores per task.
#SBATCH --time=23:00:00            # Time limit hrs:min:sec
#SBATCH --output=../logs/muril_%j.log  # Standard output and error log

pwd; hostname; date

echo "finetuning muril on $SLURM_CPUS_ON_NODE CPU cores"

python ../py/muril_param.py

date

</code></pre></div></div> <ol> <li>We have to specify the number of nodes that we want to use: <code class="language-plaintext highlighter-rouge">#SBATCH --nodes=X</code></li> <li>We have to specify the amount of GPUs per node (with a limit of 2 GPUs per user): <code class="language-plaintext highlighter-rouge">#SBATCH --gres=gpu:Y</code></li> <li>The total number of tasks is equal to the product between the number of nodes and the number of GPUs per node: <code class="language-plaintext highlighter-rouge">#SBATCH --ntasks=X·Y</code></li> <li>We have to distribute the number of tasks per node: <code class="language-plaintext highlighter-rouge">#SBATCH --tasks-per-node=(X·Y)/X</code></li> </ol> <h2 id="mpi4py">MPI4PY</h2> <p>If you have to use mpi4py package in your code, then you can put below commands in you .sh file</p> <p><code class="language-plaintext highlighter-rouge">#module load spack</code> <code class="language-plaintext highlighter-rouge">#source /home/apps/spack/share/spack/setup-env.sh</code> <code class="language-plaintext highlighter-rouge">#spack load py-mpi4py@3.1.2 /rhlwd3z</code></p> <p>Or you can individually execute these commands on your terminal one by one.</p> <h2 id="useful-commands">Useful Commands</h2> <p><code class="language-plaintext highlighter-rouge">module avail</code> <code class="language-plaintext highlighter-rouge">sacct -u “uname”</code></p>]]></content><author><name>Pritam Kadasi</name></author><category term="academics"/><category term="server"/><summary type="html"><![CDATA[a beginner's guide]]></summary></entry></feed>